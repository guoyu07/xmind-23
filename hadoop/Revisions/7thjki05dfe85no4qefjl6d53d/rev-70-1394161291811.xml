<?xml version="1.0" encoding="UTF-8" standalone="no"?><xmap-revision-content xmlns="urn:xmind:xmap:xmlns:revision:1.0" xmlns:fo="http://www.w3.org/1999/XSL/Format" xmlns:svg="http://www.w3.org/2000/svg" xmlns:xhtml="http://www.w3.org/1999/xhtml" xmlns:xlink="http://www.w3.org/1999/xlink"><sheet id="7thjki05dfe85no4qefjl6d53d" timestamp="1394161291045" xmlns="urn:xmind:xmap:xmlns:content:2.0"><topic id="1845jb1d81p4mgj9fliu21d9d7" structure-class="org.xmind.ui.map.clockwise" timestamp="1394094794185"><title>Hadoop</title><children><topics type="attached"><topic id="5ghis7pqe2k36irsplulj546ql" timestamp="1394158965348"><title>1、Hadoop简介</title><notes><html><xhtml:p>1、hadoop是一个开源，可以更容易开发和处理大数据的软件平台，它包括两部分：</xhtml:p><xhtml:p>	a、HDFS</xhtml:p><xhtml:p>	b、MapReduce</xhtml:p><xhtml:p>	提供的是云平台基础架构</xhtml:p><xhtml:p>	1.0</xhtml:p><xhtml:p>	开发分布式程序：</xhtml:p><xhtml:p/><xhtml:p>2、它是依据google的论文gfs，MapReduce模型 bigtable</xhtml:p><xhtml:p>									hadoop MapReduce HDFS</xhtml:p><xhtml:p/><xhtml:p>3、优点：</xhtml:p><xhtml:p>	可扩展、经济、可靠、高效</xhtml:p><xhtml:p/><xhtml:p>4、Pig 有一套原语，使用户不用写MapReduce程序</xhtml:p><xhtml:p>	 Hive 是一个数据仓库，提供类SQL，映射成表</xhtml:p><xhtml:p>	 HBase 是一个分布式数据库</xhtml:p><xhtml:p>	 ZooKeeper 是一个分布式的协调框架</xhtml:p><xhtml:p/><xhtml:p>5、hadoop hdfs 是一个分布式的文件系统</xhtml:p><xhtml:p>	特点：	高容错性</xhtml:p><xhtml:p>				可以部署在廉价的硬件上</xhtml:p><xhtml:p>				高吞吐量</xhtml:p><xhtml:p/><xhtml:p>6、hadoop mapreduce</xhtml:p><xhtml:p>	是将map reduce 进行自动的并行化分配，达到一定的计算能力</xhtml:p><xhtml:p/><xhtml:p>7、学习资源</xhtml:p><xhtml:p>	Hadoop的官网 http://hadoop.apache.org</xhtml:p><xhtml:p>	Cloudera的官网 http://www.cloudera.com/content/cloudera/en/home.html</xhtml:p></html><plain>1、hadoop是一个开源，可以更容易开发和处理大数据的软件平台，它包括两部分：
	a、HDFS
	b、MapReduce
	提供的是云平台基础架构
	1.0
	开发分布式程序：

2、它是依据google的论文gfs，MapReduce模型 bigtable
									hadoop MapReduce HDFS

3、优点：
	可扩展、经济、可靠、高效

4、Pig 有一套原语，使用户不用写MapReduce程序
	 Hive 是一个数据仓库，提供类SQL，映射成表
	 HBase 是一个分布式数据库
	 ZooKeeper 是一个分布式的协调框架

5、hadoop hdfs 是一个分布式的文件系统
	特点：	高容错性
				可以部署在廉价的硬件上
				高吞吐量

6、hadoop mapreduce
	是将map reduce 进行自动的并行化分配，达到一定的计算能力

7、学习资源
	Hadoop的官网 http://hadoop.apache.org
	Cloudera的官网 http://www.cloudera.com/content/cloudera/en/home.html</plain></notes><children><topics type="attached"><topic id="6olf7p9t77dsg99k8vp2qo2p96" timestamp="1394158596093"><title>1、hadoop特性</title><notes><html><xhtml:p>Append：支持文件追加功能，如果想使用HBase，OLTP（联机事务），需要这个特性</xhtml:p><xhtml:p>RAID：在保证数据可靠的前提下，通过引入校验码减少数据块数目。详细链接：https://issues.apache.org/jira/browse/HDFS/component/12313080</xhtml:p><xhtml:p>Symlink:支持HDFS文件链接，具体可参考：https://issues.apache.org/jira/browse/HDFS-245</xhtml:p><xhtml:p>Security:Hadoop安全性，具体可参考：https://issues.apache.org/jira/browse/HADOOP-4487</xhtml:p><xhtml:p>NameNode HA（high avaible），可以可参考：https://issues.apache.org/jira/browse/HDFS-1064 </xhtml:p><xhtml:p>HDFS Federation和YARN</xhtml:p><xhtml:p/></html><plain>Append：支持文件追加功能，如果想使用HBase，OLTP（联机事务），需要这个特性
RAID：在保证数据可靠的前提下，通过引入校验码减少数据块数目。详细链接：https://issues.apache.org/jira/browse/HDFS/component/12313080
Symlink:支持HDFS文件链接，具体可参考：https://issues.apache.org/jira/browse/HDFS-245
Security:Hadoop安全性，具体可参考：https://issues.apache.org/jira/browse/HADOOP-4487
NameNode HA（high avaible），可以可参考：https://issues.apache.org/jira/browse/HDFS-1064 
HDFS Federation和YARN
</plain></notes></topic><topic id="5v2oefvc99mt74oj4lptgjb2op" timestamp="1394158591747"><title>2、版本演进</title><notes><html><xhtml:p><xhtml:img xhtml:src="xap:attachments/1jagatit3igqpovnrdsbd4560l.png"/></xhtml:p></html><plain/></notes></topic><topic id="0v30rqkosarvk3bu13tuot2met" timestamp="1394158990742"><title>3、Cloudera发布版</title><notes><html><xhtml:p><xhtml:img xhtml:src="xap:attachments/75jlqplsj8bphuccrbl1s6reg5.png"/></xhtml:p></html><plain/></notes></topic></topics></children></topic><topic id="78h9uu2h418ab55dan5ocpbm0b" timestamp="1394158199004"><title>2、分布式安装</title><children><topics type="attached"><topic id="4vj9v71i1jn8odpbaqbon1lh2m" timestamp="1394087663565"><title>1、windows</title><children><topics type="attached"><topic id="5j9ffqbdj0bg2j909pk3co6urd" timestamp="1394085860880"><title>1、jdk</title></topic><topic id="7pqhoo2r61embd05spsc12b79t" timestamp="1394086235210"><title>2、cygwin linux虚拟平台</title><notes><html><xhtml:p>http://www.cygwin.com/</xhtml:p><xhtml:p><xhtml:img xhtml:src="xap:attachments/609tkcnvi0o2kjt2hk3rhgsblb.png"/></xhtml:p></html><plain>http://www.cygwin.com/
</plain></notes></topic><topic id="27lam1cf7atbnn693tgd1gqib5" timestamp="1394106368797"><title>3、安装ssh</title><notes><html><xhtml:p>#在cygwin下安装sshd服务</xhtml:p><xhtml:p>ssh-host-config</xhtml:p><xhtml:p/><xhtml:p>#启动sshd服务</xhtml:p><xhtml:p>net start sshd</xhtml:p><xhtml:p/><xhtml:p>ssh-keygen -t rsa</xhtml:p><xhtml:p/><xhtml:p>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</xhtml:p><xhtml:p/></html><plain>#在cygwin下安装sshd服务
ssh-host-config

#启动sshd服务
net start sshd

ssh-keygen -t rsa

cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
</plain></notes></topic></topics></children></topic><topic id="0nk31t2ds7ck0hb4gr0hq33tvn" timestamp="1394158200881"><title>2、linux</title><children><topics type="attached"><topic id="2egdbchgnrpk5cd3les91ggvib" timestamp="1394088283394"><title>1、jdk</title></topic><topic id="5epes924vb5k1q71ad1j94pj3d" timestamp="1394158180787"><title>2、安装ssh</title><notes><html><xhtml:p>#启动sshd服务</xhtml:p><xhtml:p>service sshd start</xhtml:p><xhtml:p/><xhtml:p>ssh-keygen -t rsa</xhtml:p><xhtml:p/><xhtml:p>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</xhtml:p><xhtml:p/></html><plain>#启动sshd服务
service sshd start

ssh-keygen -t rsa

cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
</plain></notes></topic><topic id="2tpa52qg98hfs2dluhdunh42ul" timestamp="1394158204489"><title>3、关闭防火墙</title></topic></topics></children></topic><topic id="132gsksr9pak30eisalbm90hk7" timestamp="1394155273511"><title>3、hadoop安装</title><notes><html><xhtml:p><xhtml:img xhtml:src="xap:attachments/2dnbhppv13j1km4u9mulde4off.png"/></xhtml:p><xhtml:p><xhtml:span style-id="7turfdg0su2teous0bsv0h1b05"/></xhtml:p><xhtml:p><xhtml:span style-id="7turfdg0su2teous0bsv0h1b05">export HADOOP_HOME=/home/test/hadoop-1.0.0</xhtml:span></xhtml:p><xhtml:p><xhtml:span style-id="7turfdg0su2teous0bsv0h1b05"/></xhtml:p><xhtml:p><xhtml:img xhtml:src="xap:attachments/0tc9ciatenajb17ndct3hviahj.png"/></xhtml:p></html><plain>

export HADOOP_HOME=/home/test/hadoop-1.0.0

</plain></notes><children><topics type="attached"><topic id="5g0bdml6ghef66qnh6vqed5bes" timestamp="1394106567569"><title>1、conf/hadoop-env.sh</title><notes><html><xhtml:p>配置jdk</xhtml:p><xhtml:p>export JAVA_HOME=/cygdrive/d/java/jdk1.6</xhtml:p><xhtml:p/></html><plain>配置jdk
export JAVA_HOME=/cygdrive/d/java/jdk1.6
</plain></notes></topic><topic id="56n36a7q0cndm22ksc96sigkam" timestamp="1394154928709"><title>2、conf/core-site.xml</title><notes><html><xhtml:p>&lt;configuration&gt;</xhtml:p><xhtml:p>	&lt;property&gt;</xhtml:p><xhtml:p>		&lt;name&gt;fs.default.name&lt;/name&gt;</xhtml:p><xhtml:p>		&lt;value&gt;hdfs://localhost:9000&lt;/value&gt;</xhtml:p><xhtml:p>	&lt;/property&gt;</xhtml:p><xhtml:p/><xhtml:p>	&lt;!-- 默认情况下生产的tmp目录会存在hdfs中，系统重启会清除此目录内容 --&gt;</xhtml:p><xhtml:p>	&lt;property&gt;</xhtml:p><xhtml:p>		&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</xhtml:p><xhtml:p>		&lt;value&gt;/home/wanglj/hadoop/tmp&lt;/value&gt;</xhtml:p><xhtml:p>	&lt;/property&gt;</xhtml:p><xhtml:p>&lt;/configuration&gt;</xhtml:p></html><plain>&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;fs.default.name&lt;/name&gt;
		&lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
	&lt;/property&gt;

	&lt;!-- 默认情况下生产的tmp目录会存在hdfs中，系统重启会清除此目录内容 --&gt;
	&lt;property&gt;
		&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
		&lt;value&gt;/home/wanglj/hadoop/tmp&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;</plain></notes></topic><topic id="185rme2j39dhl859pk9eeh3a0g" timestamp="1394087449160"><title>3、conf/hdfs-site.xml</title><notes><html><xhtml:p>&lt;configuration&gt;</xhtml:p><xhtml:p>	&lt;property&gt;</xhtml:p><xhtml:p>		&lt;name&gt;dfs.replication&lt;/name&gt;</xhtml:p><xhtml:p>		&lt;value&gt;1&lt;/value&gt;</xhtml:p><xhtml:p>	&lt;/property&gt;</xhtml:p><xhtml:p>&lt;/configuration&gt;</xhtml:p></html><plain>&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;dfs.replication&lt;/name&gt;
		&lt;value&gt;1&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;</plain></notes></topic><topic id="5ush1g9357taj987ehc2g3sf44" timestamp="1394087498724"><title>4、conf/mapred-site.xml</title><notes><html><xhtml:p>&lt;configuration&gt;</xhtml:p><xhtml:p>	&lt;property&gt;</xhtml:p><xhtml:p>		&lt;name&gt;mapred.job.tracker&lt;/name&gt;</xhtml:p><xhtml:p>		&lt;value&gt;localhost:9001&lt;/value&gt;</xhtml:p><xhtml:p>	&lt;/property&gt;</xhtml:p><xhtml:p>&lt;/configuration&gt;</xhtml:p></html><plain>&lt;configuration&gt;
	&lt;property&gt;
		&lt;name&gt;mapred.job.tracker&lt;/name&gt;
		&lt;value&gt;localhost:9001&lt;/value&gt;
	&lt;/property&gt;
&lt;/configuration&gt;</plain></notes></topic><topic id="6f86olp0g79a7u53j9e8t05emu" timestamp="1394155389485"><title>5、单点伪分布式配置</title><notes><html><xhtml:p>conf/masters和conf/slaves文件中默认设置均为localhost</xhtml:p><xhtml:p/><xhtml:p>其中conf/masters中的内容为namenode主机名</xhtml:p><xhtml:p>	conf/slaves中的内容为datanode主机名</xhtml:p><xhtml:p/></html><plain>conf/masters和conf/slaves文件中默认设置均为localhost

其中conf/masters中的内容为namenode主机名
	conf/slaves中的内容为datanode主机名
</plain></notes></topic></topics></children></topic><topic id="7tamn2299kqgnajhnfptc3rkg4" timestamp="1394089720644"><title>4、启动hadoop</title><notes><html><xhtml:p>1、格式化文件系统</xhtml:p><xhtml:p>	hadoop namenode -format</xhtml:p><xhtml:p/><xhtml:p>2、启动hadoop</xhtml:p><xhtml:p>	启动关闭所有任务 start-all.sh/stop-all.sh</xhtml:p><xhtml:p>	启动关闭HDFS start-dfs.sh/stop-dfs.sh</xhtml:p><xhtml:p>	启动关闭MapReduce start-mapred.sh/stop-mapred.sh</xhtml:p><xhtml:p/><xhtml:p>3、用jps命令查看进程，确保有</xhtml:p><xhtml:p>	NameNode， DataNode， JobTracker， TaskTracker</xhtml:p><xhtml:p/></html><plain>1、格式化文件系统
	hadoop namenode -format

2、启动hadoop
	启动关闭所有任务 start-all.sh/stop-all.sh
	启动关闭HDFS start-dfs.sh/stop-dfs.sh
	启动关闭MapReduce start-mapred.sh/stop-mapred.sh

3、用jps命令查看进程，确保有
	NameNode， DataNode， JobTracker， TaskTracker
</plain></notes></topic><topic id="34fmcfb4u9879e0ir9a3c6ch7s" timestamp="1394089731640"><title>5、Hadoop ui</title><notes><html><xhtml:p>		http://localhost:50070/		查看HDFS管理页面</xhtml:p><xhtml:p>		http://localhost:50030/		查看JobTracker管理页面</xhtml:p><xhtml:p>		HDFS通信端口：9000</xhtml:p><xhtml:p>		MapReduce通信端口：9001</xhtml:p><xhtml:p/></html><plain>		http://localhost:50070/		查看HDFS管理页面
		http://localhost:50030/		查看JobTracker管理页面
		HDFS通信端口：9000
		MapReduce通信端口：9001
</plain></notes></topic><topic id="6p4q8okoart1k74e5gbinlj5ne" timestamp="1394106873545"><title>6、eclipse集成开发环境</title><notes><html><xhtml:p>1、导入hadoop-core.jar 以及 hadoop/lib下的所有jar包</xhtml:p><xhtml:p>2、加入hadoop配置文件，包括conf/core-site.xml,conf/hdfs-site.xml,conf/mapred-site.xml，并修改相应参数</xhtml:p><xhtml:p/></html><plain>1、导入hadoop-core.jar 以及 hadoop/lib下的所有jar包
2、加入hadoop配置文件，包括conf/core-site.xml,conf/hdfs-site.xml,conf/mapred-site.xml，并修改相应参数
</plain></notes></topic></topics></children></topic><topic id="2hno0ig96rkim61t23rbjf0dai" timestamp="1394090878482"><title>3、hadoop shell</title><children><topics type="attached"><topic id="0f7c19fod8u5oipa1pue9uo033" timestamp="1394095299068"><title>shell命令</title><children><topics type="attached"><topic id="2nl1lreqicvpnmeio4dmhaobma" timestamp="1394095297527"><title>1、创建目录</title><notes><html><xhtml:p>hadoop fs -mkdir /tmp/input</xhtml:p></html><plain>hadoop fs -mkdir /tmp/input</plain></notes></topic><topic id="6sp3553sfla7l1v5bu8hmo44o6" timestamp="1394095561410"><title>2、拷贝文件</title><notes><html><xhtml:p>#put拷贝local文件到hdfs</xhtml:p><xhtml:p>hadoop fs -put input/* /tmp/input/</xhtml:p><xhtml:p>#</xhtml:p><xhtml:p>hadoop fs -cp tmp tmp2</xhtml:p></html><plain>#put拷贝local文件到hdfs
hadoop fs -put input/* /tmp/input/
#
hadoop fs -cp tmp tmp2</plain></notes></topic><topic id="675vm2sfpjb2d8hspkom6o2r9p" timestamp="1394090876477"><title>3、执行jar</title><notes><html><xhtml:p>hadoop jar hadoop-example.jar wordcount /tmp/input /tmp/output</xhtml:p><xhtml:p/><xhtml:p>hadoop fs -cat /tmp/output</xhtml:p><xhtml:p/><xhtml:p/></html><plain>hadoop jar hadoop-example.jar wordcount /tmp/input /tmp/output

hadoop fs -cat /tmp/output

</plain></notes></topic><topic id="77140sf15g8ttauvhoqs0er0eo" timestamp="1394096152748"><title>4、ls lsr</title><notes><html><xhtml:p>hadoop fs -help mv</xhtml:p><xhtml:p/><xhtml:p>hadoop fs -ls /tmp/input</xhtml:p><xhtml:p/><xhtml:p>#递归显示</xhtml:p><xhtml:p>hadoop fs -lsr /tmp/input</xhtml:p><xhtml:p>#显示文件大小</xhtml:p><xhtml:p>hadoop fs -du /tmp</xhtml:p><xhtml:p>#显示整个文件的大小</xhtml:p><xhtml:p>hadoop fs -dus /tmp</xhtml:p><xhtml:p>#</xhtml:p><xhtml:p>hadoop fs -count /tmp</xhtml:p><xhtml:p>hadoop fs -copyFromLocal file1 /tmp</xhtml:p><xhtml:p>hadoop fs -moveFromLocal file2 /tmp</xhtml:p><xhtml:p/></html><plain>hadoop fs -help mv

hadoop fs -ls /tmp/input

#递归显示
hadoop fs -lsr /tmp/input
#显示文件大小
hadoop fs -du /tmp
#显示整个文件的大小
hadoop fs -dus /tmp
#
hadoop fs -count /tmp
hadoop fs -copyFromLocal file1 /tmp
hadoop fs -moveFromLocal file2 /tmp
</plain></notes></topic></topics></children></topic><topic id="0an8s76d0sgj2ec7ef4kj0qhbg" timestamp="1394091754382"><title>hadoop bin目录下命令</title><children><topics type="attached"><topic id="491hthcficv026vdr6hp6q59t1" timestamp="1394093074134"><title>hadoop</title><notes><html><xhtml:p>start-dfs.sh -&gt; hadoop-daemon.sh -&gt; hadoop</xhtml:p><xhtml:p/><xhtml:p>hadoop fsch /</xhtml:p><xhtml:p/><xhtml:p>hadoop distcp "hdfs://localhost:9000/tmp/test" "hdfs://localhost:9000/tmp/test2"</xhtml:p><xhtml:p/><xhtml:p>hadoop daemonlog -getlevel 127.0.0.1:50070 namenode</xhtml:p><xhtml:p>hadoop daemonlog -setlevel 127.0.0.1:50070 namenode info</xhtml:p></html><plain>start-dfs.sh -&gt; hadoop-daemon.sh -&gt; hadoop

hadoop fsch /

hadoop distcp "hdfs://localhost:9000/tmp/test" "hdfs://localhost:9000/tmp/test2"

hadoop daemonlog -getlevel 127.0.0.1:50070 namenode
hadoop daemonlog -setlevel 127.0.0.1:50070 namenode info</plain></notes></topic><topic id="72qeki91kc9ra3u4h1fvvboe0l" timestamp="1394091085703"><title>hadoop-config.sh</title><notes><html><xhtml:p>对一些变量进行赋值</xhtml:p><xhtml:p/></html><plain>对一些变量进行赋值
</plain></notes></topic><topic id="7bh1anb2obr9fktchh1lcptidv" timestamp="1394091885629"><title>hadoop-daemon.sh</title><notes><html><xhtml:p>启动单个节点</xhtml:p><xhtml:p/><xhtml:p>hadoop-daemon.sh start/stop namenode</xhtml:p><xhtml:p/><xhtml:p>hadoop-daemon.sh start/stop datanode</xhtml:p><xhtml:p/><xhtml:p>hadoop-daemon.sh start/stop tasktracker</xhtml:p><xhtml:p/><xhtml:p/></html><plain>启动单个节点

hadoop-daemon.sh start/stop namenode

hadoop-daemon.sh start/stop datanode

hadoop-daemon.sh start/stop tasktracker

</plain></notes></topic><topic id="4l0givtfmi4tonvbua6mgagkct" timestamp="1394094791187"><title>hadoop-daemons.sh</title><notes><html><xhtml:p>在所有slaves上运行相同的脚本hadoop-daemon.sh</xhtml:p><xhtml:p/></html><plain>在所有slaves上运行相同的脚本hadoop-daemon.sh
</plain></notes></topic><topic id="503jmb2rguo0evp7biccnfuqhd" timestamp="1394091628535"><title>start-all.sh</title></topic><topic id="6ddh9iq16ccg5dus9of5ko368k" timestamp="1394091762978"><title>start-dfs.sh</title></topic><topic id="75qkg1v6r0co5ca6gf3mbcuoti" timestamp="1394091776305"><title>start-mapred.sh</title></topic><topic id="0i84jslkl9kvp6o7p3p29jm2fv" timestamp="1394091718193"><title>start-balancer.sh</title><notes><html><xhtml:p>用于负载均衡</xhtml:p></html><plain>用于负载均衡</plain></notes></topic><topic id="3os1d176tsodt4pc66tefo2ada" timestamp="1394091734646"><title>start-jobhistoryserver.sh</title></topic></topics></children></topic></topics></children><notes><html><xhtml:p><xhtml:img xhtml:src="xap:attachments/3ci8r6mtovcp7nojgojmj0k5eu.png"/></xhtml:p></html><plain/></notes></topic><topic id="5tcjp876nq7teedck0rcjoho1f" timestamp="1394161148828"><title>4、HDFS</title><notes><html><xhtml:p>提供分布式存储机制，提供可线性增长的海量存储能力</xhtml:p><xhtml:p>自动数据冗余，无须使用raid，无须另行备份</xhtml:p><xhtml:p>为进一步分析计算提供数据基础</xhtml:p><xhtml:p><xhtml:img xhtml:src="xap:attachments/0i2iv9mpse7nd8aagit6slib6d.png"/></xhtml:p></html><plain>提供分布式存储机制，提供可线性增长的海量存储能力
自动数据冗余，无须使用raid，无须另行备份
为进一步分析计算提供数据基础
</plain></notes><children><topics type="attached"><topic id="410rqggbelem1qltje0vmm8tq0" timestamp="1394109296699"><title>block块</title><notes><html><xhtml:p>默认为64M</xhtml:p><xhtml:p/></html><plain>默认为64M
</plain></notes></topic><topic id="6qcb8mr8o23cfh867d12vovrth" timestamp="1394160024673"><title>hdfs设计基础和目标</title><notes><html><xhtml:p>1、硬件错误是常态，因此需要<xhtml:span style-id="6tnvhc8mqcfbiot316h38fn622">冗余</xhtml:span></xhtml:p><xhtml:p>2、流式数据访问。即数据<xhtml:span style-id="6tnvhc8mqcfbiot316h38fn622">批量读取而非随机读写</xhtml:span>，Hadoop擅长做的是数据分析而不是事务处理</xhtml:p><xhtml:p>3、<xhtml:span style-id="6tnvhc8mqcfbiot316h38fn622">大规模</xhtml:span>数据集</xhtml:p><xhtml:p>4、简单一致性模型。为了降低系统复杂度，对文件采用一次性写多次度的逻辑设计，即是文件一经写入，关闭，就再也<xhtml:span style-id="6tnvhc8mqcfbiot316h38fn622">不能修改</xhtml:span></xhtml:p><xhtml:p>5、程序采用“数据就近”原则分配节点执行</xhtml:p><xhtml:p/></html><plain>1、硬件错误是常态，因此需要冗余
2、流式数据访问。即数据批量读取而非随机读写，Hadoop擅长做的是数据分析而不是事务处理
3、大规模数据集
4、简单一致性模型。为了降低系统复杂度，对文件采用一次性写多次度的逻辑设计，即是文件一经写入，关闭，就再也不能修改
5、程序采用“数据就近”原则分配节点执行
</plain></notes></topic><topic id="5qb1c7ljrirkqmd1bqij7ku21n" timestamp="1394160964628"><title>hdfs体系结构</title><notes><html><xhtml:p>NameNode</xhtml:p><xhtml:p>DataNode</xhtml:p><xhtml:p>事务日志</xhtml:p><xhtml:p>映像文件</xhtml:p><xhtml:p>SecondaryNameNode</xhtml:p><xhtml:p/></html><plain>NameNode
DataNode
事务日志
映像文件
SecondaryNameNode
</plain></notes><children><topics type="attached"><topic id="5go5jjmb6madnqtpp96s45kk98" timestamp="1394160963080"><title>NameNode</title><notes><html><xhtml:p>1、管理文件系统的命名空间</xhtml:p><xhtml:p>2、记录每个文件数据块在各个datanode上的位置和副本信息</xhtml:p><xhtml:p>3、协调客户端对文件的访问</xhtml:p><xhtml:p>4、记录命名空间内的改动或空间本身属性的改动</xhtml:p><xhtml:p>5、</xhtml:p></html><plain>1、管理文件系统的命名空间
2、记录每个文件数据块在各个datanode上的位置和副本信息
3、协调客户端对文件的访问
4、记录命名空间内的改动或空间本身属性的改动
5、</plain></notes></topic><topic id="0tumokjrfhfmgpuojr2eos8md1" timestamp="1394161047475"><title>DataNode</title><notes><html><xhtml:p>1、负责所在物理节点的存储管理</xhtml:p><xhtml:p>2、一次写入，多次读取（不修改）</xhtml:p><xhtml:p>3、文件由数据块组成，典型的块大小的64M</xhtml:p><xhtml:p>4、数据块尽量散布到各个节点</xhtml:p><xhtml:p/></html><plain>1、负责所在物理节点的存储管理
2、一次写入，多次读取（不修改）
3、文件由数据块组成，典型的块大小的64M
4、数据块尽量散布到各个节点
</plain></notes></topic></topics></children></topic><topic id="6qgtvboqnck1kr8qe7ihhc1ec8" timestamp="1394161291045"><title>读取数据流程</title><notes><html><xhtml:p>1、客户端要访问HDFS中的一个文件</xhtml:p><xhtml:p>2、首先从namenode获得组成这个文件的数据块位置列表</xhtml:p><xhtml:p>3、根据列表知道存储数据块的datanode</xhtml:p><xhtml:p>4、访问datanode获取数据</xhtml:p><xhtml:p>5、namenode并不参与数据实际传输</xhtml:p><xhtml:p><xhtml:img xhtml:src="xap:attachments/7g15d6dfe1iduqvdtu8mltuhqi.png"/></xhtml:p></html><plain>1、客户端要访问HDFS中的一个文件
2、首先从namenode获得组成这个文件的数据块位置列表
3、根据列表知道存储数据块的datanode
4、访问datanode获取数据
5、namenode并不参与数据实际传输
</plain></notes></topic></topics></children></topic></topics></children></topic><title>画布 1</title></sheet></xmap-revision-content>